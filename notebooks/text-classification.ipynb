{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n",
      "PyTorch version 1.6.0 available.\n",
      "TensorFlow version 2.3.0 available.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Flash and PyTorch Lightning\n",
    "from pl_flash.text import TextClassificationData, TextClassifier\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/imdb\")\n",
    "\n",
    "if not data_path.exists():\n",
    "    with urlopen(\"https://pl-flash-data.s3.amazonaws.com/imdb.zip\") as resp:\n",
    "        with ZipFile(BytesIO(resp.read())) as file:\n",
    "            file.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese indie film with humor and philosophy ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Isaac Florentine has made some of the best wes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After seeing the low-budget shittier versions ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've seen the original English version on vide...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahh, nuthin' like cheesy, explopitative, semi-...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  Japanese indie film with humor and philosophy ...  positive\n",
       "1  Isaac Florentine has made some of the best wes...  negative\n",
       "2  After seeing the low-budget shittier versions ...  negative\n",
       "3  I've seen the original English version on vide...  positive\n",
       "4  Ahh, nuthin' like cheesy, explopitative, semi-...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_path/\"train.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "Checking /home/teddy/.cache/huggingface/datasets/d927c670fd53408efaea423294f823daf050357872041f191bfea8af06a952b6.03756fef6da334f50a7ff73608e21b5018229944ca250416ce7352e25d84a552.py for additional imports.\n",
      "Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/csv/csv.py at /home/teddy/.cache/huggingface/modules/datasets_modules/datasets/csv\n",
      "Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/csv/csv.py at /home/teddy/.cache/huggingface/modules/datasets_modules/datasets/csv/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277\n",
      "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/csv/csv.py to /home/teddy/.cache/huggingface/modules/datasets_modules/datasets/csv/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/csv.py\n",
      "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/csv/dataset_infos.json\n",
      "Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/csv/csv.py at /home/teddy/.cache/huggingface/modules/datasets_modules/datasets/csv/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/csv.json\n",
      "Using custom data configuration default\n",
      "Overwrite dataset info from restored data version.\n",
      "Loading Dataset info from /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277\n",
      "Reusing dataset csv (/home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277)\n",
      "Constructing Dataset for split train, validation, test, from /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277\n",
      "100%|██████████| 3/3 [00:00<00:00, 708.74it/s]\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Loading cached processed dataset at /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/cache-58e611b9754fe092.arrow\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Loading cached processed dataset at /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/cache-58e4215b5b071b22.arrow\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Loading cached processed dataset at /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/cache-fb7a4e9ad680462c.arrow\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/cache-7a9447f8aca59275.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c14577165545abb8caeb0031f47d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 22500 examples in 99118186 bytes /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/tmpt2m67mqy.\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/cache-16a4b77e3363d2e5.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e28ddbb86154051bd54680287e1759d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 2500 examples in 11005031 bytes /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/tmpbqcwwuwh.\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/cache-8b6ae889fbd14d76.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4039a2fb24f469aaaf0f4cbcf19c13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 25000 examples in 110161107 bytes /home/teddy/.cache/huggingface/datasets/csv/default-4c551d7a04ff9804/0.0.0/0d06ce3712951dae7909fb214283b88efab3578535edb5eebd37c498b7a35277/tmp87iwmjei.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'labels'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = TextClassificationData.from_files(\n",
    "    train_file=data_path/\"train.csv\",\n",
    "    valid_file=data_path/\"valid.csv\",\n",
    "    test_file=data_path/\"test.csv\",\n",
    "    text_field=\"review\",\n",
    "    label_field=\"sentiment\",\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "task = TextClassifier(num_classes=2, metrics=pl.metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | metrics | ModuleDict                    | 0     \n",
      "1 | model   | BertForSequenceClassification | 109 M \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e080a5ccdd4dcb97db0f21a13666c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/teddy/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=5,\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "trainer.fit(task, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}