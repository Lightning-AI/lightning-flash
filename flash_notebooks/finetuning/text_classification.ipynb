{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coordinate-grounds",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PyTorchLightning/lightning-flash/blob/master/flash_notebooks/finetuning/text_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-consensus",
   "metadata": {},
   "source": [
    "In this notebook, we'll go over the basics of lightning Flash by finetunig a TextClassifier on [IMDB Dataset](https://www.imdb.com/interfaces/).\n",
    "\n",
    "# Finetuning\n",
    "\n",
    "Finetuning consists of four steps:\n",
    " \n",
    " - 1. Train a source neural network model on a source dataset. For text classication, it is traditionally  a transformer model such as BERT [Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805) trained on wikipedia.\n",
    "As those model are costly to train, [Transformers](https://github.com/huggingface/transformers) or [FairSeq](https://github.com/pytorch/fairseq) libraries provides popular pre-trained model architectures for NLP. In this notebook, we will be using [tiny-bert](https://huggingface.co/prajjwal1/bert-tiny).\n",
    "\n",
    " \n",
    " - 2. Create a new neural network the target model. Its architecture replicates all model designs and their parameters on the source model, expect the latest layer which is removed. This model without its latest layers is traditionally called a backbone\n",
    " \n",
    "\n",
    "- 3. Add new layers after the backbone where the latest output size is the number of target dataset categories. Those new layers, traditionally called head, will be randomly initialized while backbone will conserve its pre-trained weights from ImageNet.\n",
    " \n",
    "\n",
    "- 4. Train the target model on a target dataset, such as IMDB Dataset to learn to predict the associated sentiment of movie reviews. At training start, the backbone will be frozen, meaning its parameters won't be updated. Only the model head will be trained to between negative and positive reviews. On reaching first finetuning milestone, the backbone latest layers will be unfrozen and start to be trained. On reaching the second finetuning milestone, the remaining layers of the backend will be unfrozen and the entire model will be trained. In Flash, `unfreeze_milestones` controls those milestone and be used as such `trainer.finetune(..., unfreeze_milestones=(first_milestone, second_milestone))`.\n",
    "\n",
    "---\n",
    "  - Give us a ‚≠ê [on Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n",
    "  - Check out [Flash documentation](https://lightning-flash.readthedocs.io/en/latest/)\n",
    "  - Check out [Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "  - Join us [on Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-f6bl2l0l-JYMK3tbAgAmGRrlNr00f1A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-reggae",
   "metadata": {},
   "source": [
    "### Setup  \n",
    "Lightning Flash is easy to install. Simply ```pip install lightning-flash```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install lightning-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "from flash.core.data import download_data\n",
    "from flash.text import TextClassificationData, TextClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-amino",
   "metadata": {},
   "source": [
    "###  1. Download the data\n",
    "The data are downloaded from a URL, and save in a 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(\"https://pl-flash-data.s3.amazonaws.com/imdb.zip\", 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-positive",
   "metadata": {},
   "source": [
    "<h2>2. Load the data</h2>\n",
    "\n",
    "Flash Tasks have built-in DataModules that you can abuse to organize your data. Pass in a train, validation and test folders and Flash will take care of the rest.\n",
    "Creates a TextClassificationData object from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = TextClassificationData.from_files(\n",
    "    train_file=\"data/imdb/train.csv\",\n",
    "    valid_file=\"data/imdb/valid.csv\",\n",
    "    test_file=\"data/imdb/test.csv\",\n",
    "    input=\"review\",\n",
    "    target=\"sentiment\",\n",
    "    batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-surveillance",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "###  3. Build the model\n",
    "\n",
    "Create the TextClassifier task. By default, the TextClassifier task uses a [tiny-bert](https://huggingface.co/prajjwal1/bert-tiny) backbone to train or finetune your model demo. You could use any models from [transformers - Text Classification](https://huggingface.co/models?filter=text-classification,pytorch)\n",
    "\n",
    "Backbone can easily be changed with such as `TextClassifier(backbone='bert-tiny-mnli')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(num_classes=datamodule.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-fiber",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "###  4. Create the trainer. Run once on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = flash.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-anderson",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "###  5. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.finetune(model, datamodule=datamodule, unfreeze_milestones=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-butler",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "###  6. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-acquisition",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "###  7. Save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"text_classification_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-rapid",
   "metadata": {},
   "source": [
    "<code style=\"color:#792ee5;\">\n",
    "    <h1> <strong> Congratulations - Time to Join the Community! </strong>  </h1>\n",
    "</code>\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed it and would like to join the Lightning movement, you can do so in the following ways!\n",
    "\n",
    "### Help us build Flash by adding support for new data-types and new tasks.\n",
    "Flash aims at becoming the first task hub, so anyone can get started to great amazing application using deep learning. \n",
    "If you are interested, please open a PR with your contributions !!! \n",
    "\n",
    "\n",
    "### Star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) on GitHub\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "* Please, star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "\n",
    "### Join our [Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-f6bl2l0l-JYMK3tbAgAmGRrlNr00f1A)!\n",
    "The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself and share your interests in `#general` channel\n",
    "\n",
    "### Interested by SOTA AI models ! Check out [Bolt](https://github.com/PyTorchLightning/pytorch-lightning-bolts)\n",
    "Bolts has a collection of state-of-the-art models, all implemented in [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) and can be easily integrated within your own projects.\n",
    "\n",
    "* Please, star [Bolt](https://github.com/PyTorchLightning/pytorch-lightning-bolts)\n",
    "\n",
    "### Contributions !\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) or [Bolt](https://github.com/PyTorchLightning/pytorch-lightning-bolts) GitHub Issues page and filter for \"good first issue\". \n",
    "\n",
    "* [Lightning good first issue](https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* [Bolt good first issue](https://github.com/PyTorchLightning/pytorch-lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* You can also contribute your own notebooks with useful examples !\n",
    "\n",
    "### Great thanks from the entire Pytorch Lightning Team for your interest !\n",
    "\n",
    "<img src=\"https://github.com/PyTorchLightning/lightning-flash/blob/master/docs/source/_images/flash_logo.png?raw=true\" width=\"800\" height=\"200\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
