{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "determined-vinyl",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/PyTorchLightning/lightning-flash/blob/master/flash_notebooks/generic_task.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-alfred",
   "metadata": {},
   "source": [
    "In this notebook, we'll go over the basics of lightning Flash by creating a ClassificationTask with a custom Convolutional Model and train it on [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "---\n",
    "  - Give us a ⭐ [on Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n",
    "  - Check out [Flash documentation](https://lightning-flash.readthedocs.io/en/latest/)\n",
    "  - Check out [Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "  - Join us [on Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-f6bl2l0l-JYMK3tbAgAmGRrlNr00f1A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-produce",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install lightning-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "foreign-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from flash.core.classification import ClassificationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mathematical-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "\n",
    "# TorchVision hotfix https://github.com/pytorch/vision/issues/1938\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-scenario",
   "metadata": {},
   "source": [
    "### 1. Load a basic backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "selective-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-african",
   "metadata": {},
   "source": [
    "### 2. Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stunning-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('./data', download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-teach",
   "metadata": {},
   "source": [
    "### 3. Split the data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hispanic-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test, predict = random_split(dataset, [50000, 5000, 4975, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-flour",
   "metadata": {},
   "source": [
    "### 4. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "literary-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassificationTask(model, loss_fn=nn.functional.cross_entropy, optimizer=optim.Adam, learning_rate=10e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-seller",
   "metadata": {},
   "source": [
    "### 5. Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "public-berlin",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/edgar/software/lightning-flash/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    limit_train_batches=128,\n",
    "    limit_val_batches=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-soccer",
   "metadata": {},
   "source": [
    "### 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "neural-genre",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 101 K \n",
      "1 | metrics | ModuleDict | 0     \n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "Epoch 0:  14%|█▎        | 35/256 [00:00<00:01, 217.40it/s, loss=2.4, v_num=26, val_accuracy=0.000, val_cross_entropy=2.280, train_accuracy_step=0.000, train_cross_entropy_step=2.130] /home/edgar/software/lightning-flash/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/edgar/software/lightning-flash/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  50%|█████     | 128/256 [00:00<00:00, 203.95it/s, loss=1.68, v_num=26, val_accuracy=0.000, val_cross_entropy=2.280, train_accuracy_step=0.000, train_cross_entropy_step=4.150]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  52%|█████▏    | 132/256 [00:00<00:00, 202.98it/s, loss=1.68, v_num=26, val_accuracy=0.000, val_cross_entropy=2.280, train_accuracy_step=0.000, train_cross_entropy_step=4.150]\n",
      "Epoch 0:  74%|███████▍  | 189/256 [00:00<00:00, 251.55it/s, loss=1.68, v_num=26, val_accuracy=0.000, val_cross_entropy=2.280, train_accuracy_step=0.000, train_cross_entropy_step=4.150]\n",
      "Epoch 0: 100%|██████████| 256/256 [00:00<00:00, 297.31it/s, loss=1.68, v_num=26, val_accuracy=0.359, val_cross_entropy=1.690, train_accuracy_step=0.000, train_cross_entropy_step=2.510, train_accuracy_epoch=0.344, train_cross_entropy_epoch=2.050]\n",
      "Epoch 1:  50%|█████     | 128/256 [00:00<00:00, 182.66it/s, loss=0.809, v_num=26, val_accuracy=0.359, val_cross_entropy=1.690, train_accuracy_step=1.000, train_cross_entropy_step=0.851, train_accuracy_epoch=0.344, train_cross_entropy_epoch=2.050]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 193/256 [00:00<00:00, 241.14it/s, loss=0.809, v_num=26, val_accuracy=0.359, val_cross_entropy=1.690, train_accuracy_step=1.000, train_cross_entropy_step=0.851, train_accuracy_epoch=0.344, train_cross_entropy_epoch=2.050]\n",
      "Epoch 1: 100%|██████████| 256/256 [00:00<00:00, 288.24it/s, loss=0.809, v_num=26, val_accuracy=0.484, val_cross_entropy=1.680, train_accuracy_step=0.000, train_cross_entropy_step=2.670, train_accuracy_epoch=0.633, train_cross_entropy_epoch=1.090]\n",
      "Epoch 2:  50%|█████     | 128/256 [00:00<00:00, 194.30it/s, loss=0.458, v_num=26, val_accuracy=0.484, val_cross_entropy=1.680, train_accuracy_step=1.000, train_cross_entropy_step=0.294, train_accuracy_epoch=0.633, train_cross_entropy_epoch=1.090]   \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  51%|█████     | 130/256 [00:00<00:00, 188.73it/s, loss=0.458, v_num=26, val_accuracy=0.484, val_cross_entropy=1.680, train_accuracy_step=1.000, train_cross_entropy_step=0.294, train_accuracy_epoch=0.633, train_cross_entropy_epoch=1.090]\n",
      "Epoch 2: 100%|██████████| 256/256 [00:00<00:00, 294.59it/s, loss=0.458, v_num=26, val_accuracy=0.508, val_cross_entropy=1.850, train_accuracy_step=1.000, train_cross_entropy_step=0.649, train_accuracy_epoch=0.789, train_cross_entropy_epoch=0.549]\n",
      "Epoch 3:  50%|█████     | 128/256 [00:00<00:00, 200.81it/s, loss=0.358, v_num=26, val_accuracy=0.508, val_cross_entropy=1.850, train_accuracy_step=1.000, train_cross_entropy_step=0.0079, train_accuracy_epoch=0.789, train_cross_entropy_epoch=0.549] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 144/256 [00:00<00:00, 215.08it/s, loss=0.358, v_num=26, val_accuracy=0.508, val_cross_entropy=1.850, train_accuracy_step=1.000, train_cross_entropy_step=0.0079, train_accuracy_epoch=0.789, train_cross_entropy_epoch=0.549]\n",
      "Epoch 3: 100%|██████████| 256/256 [00:00<00:00, 309.92it/s, loss=0.358, v_num=26, val_accuracy=0.602, val_cross_entropy=1.670, train_accuracy_step=0.000, train_cross_entropy_step=0.726, train_accuracy_epoch=0.875, train_cross_entropy_epoch=0.473] \n",
      "Epoch 4:  50%|█████     | 128/256 [00:00<00:00, 209.98it/s, loss=0.166, v_num=26, val_accuracy=0.602, val_cross_entropy=1.670, train_accuracy_step=1.000, train_cross_entropy_step=0.105, train_accuracy_epoch=0.875, train_cross_entropy_epoch=0.473] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4:  57%|█████▋    | 146/256 [00:00<00:00, 225.27it/s, loss=0.166, v_num=26, val_accuracy=0.602, val_cross_entropy=1.670, train_accuracy_step=1.000, train_cross_entropy_step=0.105, train_accuracy_epoch=0.875, train_cross_entropy_epoch=0.473]\n",
      "Epoch 4: 100%|██████████| 256/256 [00:00<00:00, 317.78it/s, loss=0.166, v_num=26, val_accuracy=0.641, val_cross_entropy=1.500, train_accuracy_step=1.000, train_cross_entropy_step=0.00293, train_accuracy_epoch=0.938, train_cross_entropy_epoch=0.210]\n",
      "Epoch 5:  50%|█████     | 128/256 [00:00<00:00, 185.66it/s, loss=0.0797, v_num=26, val_accuracy=0.641, val_cross_entropy=1.500, train_accuracy_step=1.000, train_cross_entropy_step=0.0154, train_accuracy_epoch=0.938, train_cross_entropy_epoch=0.210]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  57%|█████▋    | 146/256 [00:00<00:00, 199.74it/s, loss=0.0797, v_num=26, val_accuracy=0.641, val_cross_entropy=1.500, train_accuracy_step=1.000, train_cross_entropy_step=0.0154, train_accuracy_epoch=0.938, train_cross_entropy_epoch=0.210]\n",
      "Epoch 5: 100%|██████████| 256/256 [00:00<00:00, 284.90it/s, loss=0.0797, v_num=26, val_accuracy=0.586, val_cross_entropy=2.010, train_accuracy_step=1.000, train_cross_entropy_step=0.00443, train_accuracy_epoch=0.953, train_cross_entropy_epoch=0.119]\n",
      "Epoch 6:  50%|█████     | 128/256 [00:00<00:00, 178.55it/s, loss=0.0912, v_num=26, val_accuracy=0.586, val_cross_entropy=2.010, train_accuracy_step=1.000, train_cross_entropy_step=0.0729, train_accuracy_epoch=0.953, train_cross_entropy_epoch=0.119]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6:  57%|█████▋    | 146/256 [00:00<00:00, 191.48it/s, loss=0.0912, v_num=26, val_accuracy=0.586, val_cross_entropy=2.010, train_accuracy_step=1.000, train_cross_entropy_step=0.0729, train_accuracy_epoch=0.953, train_cross_entropy_epoch=0.119]\n",
      "Epoch 6: 100%|██████████| 256/256 [00:00<00:00, 274.04it/s, loss=0.0912, v_num=26, val_accuracy=0.594, val_cross_entropy=2.020, train_accuracy_step=1.000, train_cross_entropy_step=0.129, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.146] \n",
      "Epoch 7:  50%|█████     | 128/256 [00:00<00:00, 169.42it/s, loss=0.106, v_num=26, val_accuracy=0.594, val_cross_entropy=2.020, train_accuracy_step=1.000, train_cross_entropy_step=0.051, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.146]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7:  57%|█████▋    | 146/256 [00:00<00:00, 182.27it/s, loss=0.106, v_num=26, val_accuracy=0.594, val_cross_entropy=2.020, train_accuracy_step=1.000, train_cross_entropy_step=0.051, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.146]\n",
      "Epoch 7:  86%|████████▌ | 219/256 [00:00<00:00, 237.78it/s, loss=0.106, v_num=26, val_accuracy=0.594, val_cross_entropy=2.020, train_accuracy_step=1.000, train_cross_entropy_step=0.051, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.146]\n",
      "Epoch 7: 100%|██████████| 256/256 [00:00<00:00, 259.69it/s, loss=0.106, v_num=26, val_accuracy=0.586, val_cross_entropy=2.420, train_accuracy_step=1.000, train_cross_entropy_step=8.46e-6, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.178]\n",
      "Epoch 8:  50%|█████     | 128/256 [00:00<00:00, 170.78it/s, loss=0.068, v_num=26, val_accuracy=0.586, val_cross_entropy=2.420, train_accuracy_step=1.000, train_cross_entropy_step=0.00895, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.178]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8:  57%|█████▋    | 146/256 [00:00<00:00, 183.98it/s, loss=0.068, v_num=26, val_accuracy=0.586, val_cross_entropy=2.420, train_accuracy_step=1.000, train_cross_entropy_step=0.00895, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.178]\n",
      "Epoch 8:  86%|████████▌ | 219/256 [00:00<00:00, 240.22it/s, loss=0.068, v_num=26, val_accuracy=0.586, val_cross_entropy=2.420, train_accuracy_step=1.000, train_cross_entropy_step=0.00895, train_accuracy_epoch=0.945, train_cross_entropy_epoch=0.178]\n",
      "Epoch 8: 100%|██████████| 256/256 [00:00<00:00, 262.77it/s, loss=0.068, v_num=26, val_accuracy=0.656, val_cross_entropy=1.890, train_accuracy_step=1.000, train_cross_entropy_step=3.76e-5, train_accuracy_epoch=0.953, train_cross_entropy_epoch=0.150]\n",
      "Epoch 9:  50%|█████     | 128/256 [00:00<00:00, 195.85it/s, loss=0.0109, v_num=26, val_accuracy=0.656, val_cross_entropy=1.890, train_accuracy_step=1.000, train_cross_entropy_step=0.00395, train_accuracy_epoch=0.953, train_cross_entropy_epoch=0.150]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9:  57%|█████▋    | 146/256 [00:00<00:00, 208.94it/s, loss=0.0109, v_num=26, val_accuracy=0.656, val_cross_entropy=1.890, train_accuracy_step=1.000, train_cross_entropy_step=0.00395, train_accuracy_epoch=0.953, train_cross_entropy_epoch=0.150]\n",
      "Epoch 9: 100%|██████████| 256/256 [00:00<00:00, 299.16it/s, loss=0.0109, v_num=26, val_accuracy=0.641, val_cross_entropy=2.290, train_accuracy_step=1.000, train_cross_entropy_step=2.74e-5, train_accuracy_epoch=0.992, train_cross_entropy_epoch=0.0366]\n",
      "Epoch 9: 100%|██████████| 256/256 [00:00<00:00, 297.26it/s, loss=0.0109, v_num=26, val_accuracy=0.641, val_cross_entropy=2.290, train_accuracy_step=1.000, train_cross_entropy_step=2.74e-5, train_accuracy_epoch=0.992, train_cross_entropy_epoch=0.0366]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "trainer.fit(classifier, DataLoader(train), DataLoader(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-geneva",
   "metadata": {},
   "source": [
    "### 7. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ideal-johnson",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing:   1%|          | 58/4975 [00:00<00:08, 575.00it/s]/home/edgar/software/lightning-flash/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|██████████| 4975/4975 [00:06<00:00, 768.58it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.72120600938797, 'test_cross_entropy': 1.481386661529541}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test(classifier, test_dataloaders=DataLoader(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-faculty",
   "metadata": {},
   "source": [
    "<code style=\"color:#792ee5;\">\n",
    "    <h1> <strong> Congratulations - Time to Join the Community! </strong>  </h1>\n",
    "</code>\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed it and would like to join the Lightning movement, you can do so in the following ways!\n",
    "\n",
    "### Help us build Flash by adding support for new data-types and new tasks.\n",
    "Flash aims at becoming the first task hub, so anyone can get started to great amazing application using deep learning. \n",
    "If you are interested, please open a PR with your contributions !!! \n",
    "\n",
    "\n",
    "### Star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) on GitHub\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "* Please, star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "\n",
    "### Join our [Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-f6bl2l0l-JYMK3tbAgAmGRrlNr00f1A)!\n",
    "The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself and share your interests in `#general` channel\n",
    "\n",
    "### Interested by SOTA AI models ! Check out [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "Bolts has a collection of state-of-the-art models, all implemented in [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) and can be easily integrated within your own projects.\n",
    "\n",
    "* Please, star [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "\n",
    "### Contributions !\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) or [Bolt](https://github.com/PyTorchLightning/lightning-bolts) GitHub Issues page and filter for \"good first issue\". \n",
    "\n",
    "* [Lightning good first issue](https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* [Bolt good first issue](https://github.com/PyTorchLightning/lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* You can also contribute your own notebooks with useful examples !\n",
    "\n",
    "### Great thanks from the entire Pytorch Lightning Team for your interest !\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PyTorchLightning/lightning-flash/18c591747e40a0ad862d4f82943d209b8cc25358/docs/source/_static/images/logo.svg\" width=\"800\" height=\"200\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0db7cb6f8b99581485a229201158c5b3d87043825d76fdd729161e90706f71f5a",
   "display_name": "Python 3.8.5 64-bit ('.venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
